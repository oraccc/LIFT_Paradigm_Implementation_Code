# Training job submission via AzureML CLI v2

$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json

command: >-
  bash start-deepspeed.sh 8 
  --force_multi 
  train_freeform_lora.py 
  --datastore_path ${{outputs.datastore_path}} 
  --blob_data_path "path/to/instruction/dataset/on/azure/storage/blob"
  --checkpoint_dir "path/to/save/checkpoints/on/azure/storage/blob"
  --num_train_epochs 5
  --model_max_length 2048 
  --per_device_train_batch_size 8 
  --per_device_eval_batch_size 1 
  --gradient_accumulation_steps 1 
  --evaluation_strategy "no" 
  --save_strategy "steps" 
  --save_steps 200 
  --learning_rate 2e-5 
  --warmup_steps 10 
  --logging_steps 1 
  --gradient_checkpointing True 
  --fp16 True
  --deepspeed deepspeed_config.json 

experiment_name: DeepSpeed-Distributed-Training-Mistral-7B
display_name: finetuning_mistral
code: src
environment: azureml:`your_environment_on_azureml`
environment_variables:
  AZUREML_COMPUTE_USE_COMMON_RUNTIME: "True"
  AZUREML_COMMON_RUNTIME_USE_INTERACTIVE_CAPABILITY: "True"
  AZUREML_SSH_KEY: "generated-key"
outputs:
  datastore_path:
    type: uri_folder
    mode: rw_mount
    path: azureml://subscriptions/xxxxx/resourcegroups/xxxxx/workspaces/xxxxx/datastores/xxxxx/paths/
compute: azureml:`your_cluster_compute_on_azureml`
distribution:
  type: pytorch
  process_count_per_instance: 8
resources:
  instance_count: 4